{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test notebook for parsing and chunking text data.",
   "id": "ac7c6440bd4b5f21"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T23:26:51.515658Z",
     "start_time": "2025-08-23T23:26:51.510369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import pandas as pd"
   ],
   "id": "f333fbec3d873888",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-23T23:10:51.605956Z",
     "start_time": "2025-08-23T23:10:51.582152Z"
    }
   },
   "source": [
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "soup.prettify() # Pretty print the parsed HTML"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T23:14:07.392433Z",
     "start_time": "2025-08-23T23:14:07.379281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ways to navigate the parse tree\n",
    "print(soup.title)\n",
    "print(soup.title.name)\n",
    "print(soup.title.string)\n",
    "print(soup.title.parent.name)\n",
    "print(soup.p)\n",
    "print(soup.p['class'])\n",
    "print(soup.a)\n",
    "print(soup.find_all('a'))\n",
    "print(soup.find(id=\"link3\"))"
   ],
   "id": "bd866b777a6afc50",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>The Dormouse's story</title>\n",
      "title\n",
      "The Dormouse's story\n",
      "head\n",
      "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
      "['title']\n",
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
      "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
      "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Try to do the same with a local text file.",
   "id": "d9e7a3a0e2fb8f1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T22:12:32.501577Z",
     "start_time": "2025-08-24T22:12:32.475114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "df = pd.read_csv(\"../data/raw/test.csv\", header=None)\n",
    "\n",
    "all_text = \"\".join(df[0].tolist())\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(all_text)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(chunk)\n",
    "    print(\"\\n---\\n\")\n"
   ],
   "id": "d1d6722ee872ef55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval-Augmented Generation (RAG) is a technique for enhancing the accuracy and reliability of\n",
      "\n",
      "---\n",
      "\n",
      "of large language models (LLMs) with facts fetched from external sources. It is a powerful approach\n",
      "\n",
      "---\n",
      "\n",
      "approach for building chatbots and question-answering systems.The first step in a RAG pipeline is\n",
      "\n",
      "---\n",
      "\n",
      "is data ingestion and processing. This involves loading documents from various sources, such as\n",
      "\n",
      "---\n",
      "\n",
      "such as text files, PDFs, or websites. After loading, the documents are split into smaller,\n",
      "\n",
      "---\n",
      "\n",
      "smaller, manageable chunks.These chunks are then converted into numerical representations called\n",
      "\n",
      "---\n",
      "\n",
      "called embeddings using a sentence-transformer model. The embeddings are stored in a specialized\n",
      "\n",
      "---\n",
      "\n",
      "vector database, which allows for efficient similarity search.When a user asks a question, their\n",
      "\n",
      "---\n",
      "\n",
      "their query is also converted into an embedding. The system then searches the vector database to\n",
      "\n",
      "---\n",
      "\n",
      "to find the text chunks with embeddings most similar to the query's embedding. These chunks provide\n",
      "\n",
      "---\n",
      "\n",
      "provide relevant context for the final answer.\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
