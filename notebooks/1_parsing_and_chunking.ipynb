{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test notebook for parsing and chunking text data.",
   "id": "ac7c6440bd4b5f21"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T23:26:51.515658Z",
     "start_time": "2025-08-23T23:26:51.510369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import pandas as pd"
   ],
   "id": "f333fbec3d873888",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-23T23:10:51.605956Z",
     "start_time": "2025-08-23T23:10:51.582152Z"
    }
   },
   "source": [
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "soup.prettify() # Pretty print the parsed HTML"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T23:14:07.392433Z",
     "start_time": "2025-08-23T23:14:07.379281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ways to navigate the parse tree\n",
    "print(soup.title)\n",
    "print(soup.title.name)\n",
    "print(soup.title.string)\n",
    "print(soup.title.parent.name)\n",
    "print(soup.p)\n",
    "print(soup.p['class'])\n",
    "print(soup.a)\n",
    "print(soup.find_all('a'))\n",
    "print(soup.find(id=\"link3\"))"
   ],
   "id": "bd866b777a6afc50",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>The Dormouse's story</title>\n",
      "title\n",
      "The Dormouse's story\n",
      "head\n",
      "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
      "['title']\n",
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
      "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
      "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Try to do the same with a local text file.",
   "id": "d9e7a3a0e2fb8f1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T23:32:37.798757Z",
     "start_time": "2025-08-23T23:32:37.780749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "df = pd.read_csv(\"../data/raw/test.csv\", header=None)\n",
    "\n",
    "all_text = \"\".join(df[0].tolist())\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(all_text)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(chunk)\n",
    "    print(\"\\n---\\n\")\n"
   ],
   "id": "d1d6722ee872ef55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval-Augmented Generation (RAG) is a technique for enhancing the accuracy and reliability of large language models (LLMs) with facts fetched from external sources. It is a powerful approach for\n",
      "\n",
      "---\n",
      "\n",
      "approach for building chatbots and question-answering systems.The first step in a RAG pipeline is data ingestion and processing. This involves loading documents from various sources, such as text\n",
      "\n",
      "---\n",
      "\n",
      "such as text files, PDFs, or websites. After loading, the documents are split into smaller, manageable chunks.These chunks are then converted into numerical representations called embeddings using a\n",
      "\n",
      "---\n",
      "\n",
      "embeddings using a sentence-transformer model. The embeddings are stored in a specialized vector database, which allows for efficient similarity search.When a user asks a question, their query is\n",
      "\n",
      "---\n",
      "\n",
      "their query is also converted into an embedding. The system then searches the vector database to find the text chunks with embeddings most similar to the query's embedding. These chunks provide\n",
      "\n",
      "---\n",
      "\n",
      "chunks provide relevant context for the final answer.\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
