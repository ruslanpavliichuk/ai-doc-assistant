<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/app.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/app.py" />
              <option name="originalContent" value="import streamlit as st&#10;import os&#10;from dotenv import load_dotenv&#10;import google.generativeai as genai&#10;from pathlib import Path&#10;import tempfile&#10;&#10;# Import project modules&#10;from src.data_processing.parser import parse_document&#10;from src.data_processing.chunker import TokenChunker&#10;from src.core.embeddings import configure_gemini as configure_gemini_embeddings, embed_chunks, embed_text&#10;from src.core.vector_store import VectorStore&#10;&#10;# Load environment variables&#10;load_dotenv(&quot;.env.local&quot;)&#10;&#10;# ---- Streamlit Setup ---- #&#10;st.set_page_config(layout=&quot;wide&quot;, page_title=&quot;AI Documentation Assistant&quot;)&#10;st.title(&quot; AI Documentation Assistant&quot;)&#10;&#10;# ---- Configure Gemini ---- #&#10;@st.cache_resource&#10;def configure_gemini():&#10;    api_key = os.environ.get(&quot;GOOGLE_API_KEY&quot;)&#10;    if not api_key:&#10;        st.error(&quot;⚠️ GOOGLE_API_KEY environment variable not set. Please add it to your .env.local file.&quot;)&#10;        st.stop()&#10;    genai.configure(api_key=api_key)&#10;    configure_gemini_embeddings(api_key)  # Also configure for embeddings&#10;    return genai.GenerativeModel('gemini-2.5-pro')&#10;&#10;model = configure_gemini()&#10;&#10;# ---- Initialize Vector Store (Step 3.2) ---- #&#10;@st.cache_resource&#10;def get_vector_store():&#10;    &quot;&quot;&quot;&#10;    Initialize ChromaDB with:&#10;    - Vector dimension: 768 (Gemini text-embedding-004)&#10;    - Distance metric: Cosine similarity&#10;    &quot;&quot;&quot;&#10;    return VectorStore(collection_name=&quot;documents&quot;, persist_directory=&quot;./chroma_db&quot;)&#10;&#10;vector_store = get_vector_store()&#10;&#10;# ---- Sidebar Settings ---- #&#10;st.sidebar.header(&quot;⚙️ Settings&quot;)&#10;MAX_HISTORY = st.sidebar.number_input(&#10;    &quot;Max History Messages&quot;,&#10;    min_value=2,&#10;    max_value=20,&#10;    value=10,&#10;    step=2,&#10;    help=&quot;Number of messages to keep in chat history&quot;&#10;)&#10;&#10;st.sidebar.divider()&#10;&#10;# ---- Document Upload Section ---- #&#10;st.sidebar.header(&quot; Document Upload&quot;)&#10;uploaded_file = st.sidebar.file_uploader(&#10;    &quot;Upload documentation (PDF or HTML)&quot;,&#10;    type=[&quot;pdf&quot;, &quot;html&quot;],&#10;    help=&quot;Upload a document to add to the knowledge base&quot;&#10;)&#10;&#10;if uploaded_file is not None:&#10;    if st.sidebar.button(&quot; Process Document&quot;):&#10;        with st.sidebar.status(&quot;Processing document...&quot;, expanded=True) as status:&#10;            try:&#10;                # Save uploaded file temporarily&#10;                with tempfile.NamedTemporaryFile(delete=False, suffix=Path(uploaded_file.name).suffix) as tmp_file:&#10;                    tmp_file.write(uploaded_file.getvalue())&#10;                    tmp_path = tmp_file.name&#10;&#10;                st.write(&quot; Parsing document...&quot;)&#10;                # Step 1: Parse document&#10;                text = parse_document(tmp_path)&#10;&#10;                st.write(&quot;✂️ Chunking text...&quot;)&#10;                # Step 2: Chunk the text with metadata&#10;                chunker = TokenChunker(strategy=&quot;tokens&quot;)&#10;                chunks = chunker.chunk(&#10;                    text=text,&#10;                    chunk_size=512,&#10;                    chunk_overlap=50,&#10;                    source_id=uploaded_file.name,&#10;                    source_path=uploaded_file.name&#10;                )&#10;&#10;                st.write(f&quot;Created {len(chunks)} chunks&quot;)&#10;&#10;                st.write(&quot; Generating embeddings...&quot;)&#10;                # Step 3: Generate embeddings (768-dimensional vectors)&#10;                embeddings = embed_chunks(chunks)&#10;&#10;                st.write(&quot; Storing in vector database...&quot;)&#10;                # Step 3.3: Store in ChromaDB with cosine similarity&#10;                ids = vector_store.add_chunks(chunks, embeddings)&#10;&#10;                # Clean up temporary file&#10;                os.unlink(tmp_path)&#10;&#10;                status.update(label=f&quot;✅ Successfully processed {uploaded_file.name}!&quot;, state=&quot;complete&quot;)&#10;                st.sidebar.success(f&quot;Added {len(chunks)} chunks to knowledge base!&quot;)&#10;&#10;            except Exception as e:&#10;                status.update(label=&quot;❌ Error processing document&quot;, state=&quot;error&quot;)&#10;                st.sidebar.error(f&quot;Error: {str(e)}&quot;)&#10;&#10;st.sidebar.divider()&#10;&#10;if st.sidebar.button(&quot;️ Clear Chat History&quot;):&#10;    st.session_state.chat_history = []&#10;    st.rerun()&#10;&#10;# ---- Initialize Chat History ---- #&#10;if &quot;chat_history&quot; not in st.session_state:&#10;    st.session_state.chat_history = []&#10;&#10;# ---- Display Chat History ---- #&#10;for msg in st.session_state.chat_history:&#10;    with st.chat_message(msg[&quot;role&quot;]):&#10;        st.markdown(msg[&quot;content&quot;])&#10;        # Show sources if available&#10;        if msg[&quot;role&quot;] == &quot;assistant&quot; and &quot;sources&quot; in msg:&#10;            with st.expander(&quot; Sources&quot;):&#10;                for i, source in enumerate(msg[&quot;sources&quot;], 1):&#10;                    st.markdown(f&quot;**Source {i}:** (Similarity: {source['similarity']:.2%})&quot;)&#10;                    st.text(f&quot; Document: {source['metadata'].get('source_id', 'Unknown')}&quot;)&#10;                    st.text(f&quot; Chunk: {source['metadata'].get('chunk_index', 'N/A')}&quot;)&#10;                    st.markdown(f&quot;```\n{source['text'][:300]}...\n```&quot;)&#10;&#10;# ---- Trim Function (Removes Oldest Messages) ---- #&#10;def trim_history():&#10;    &quot;&quot;&quot;Keep only the most recent MAX_HISTORY messages&quot;&quot;&quot;&#10;    if len(st.session_state.chat_history) &gt; MAX_HISTORY:&#10;        st.session_state.chat_history = st.session_state.chat_history[-MAX_HISTORY:]&#10;&#10;# ---- RAG Function ---- #&#10;def get_relevant_context(query: str, top_k: int = 3) -&gt; tuple:&#10;    &quot;&quot;&quot;&#10;    Retrieve relevant context from vector store using RAG.&#10;    Uses cosine similarity to find most relevant chunks.&#10;&#10;    Returns:&#10;        tuple: (context_text, sources_list)&#10;    &quot;&quot;&quot;&#10;    try:&#10;        # Generate embedding for the query (768-dimensional)&#10;        query_embedding = embed_text(query, task_type=&quot;retrieval_query&quot;)&#10;&#10;        # Search vector store using cosine similarity&#10;        results = vector_store.search(query_embedding, top_k=top_k)&#10;&#10;        if not results:&#10;            return &quot;&quot;, []&#10;&#10;        # Format context&#10;        context_parts = []&#10;        sources = []&#10;&#10;        for i, (text, similarity, metadata) in enumerate(results, 1):&#10;            context_parts.append(f&quot;[Context {i}]:\n{text}\n&quot;)&#10;            sources.append({&#10;                &quot;text&quot;: text,&#10;                &quot;similarity&quot;: similarity,&#10;                &quot;metadata&quot;: metadata&#10;            })&#10;&#10;        context = &quot;\n&quot;.join(context_parts)&#10;        return context, sources&#10;&#10;    except Exception as e:&#10;        st.error(f&quot;Error retrieving context: {str(e)}&quot;)&#10;        return &quot;&quot;, []&#10;&#10;# ---- Handle User Input ---- #&#10;if prompt := st.chat_input(&quot;Ask me anything about your documentation...&quot;):&#10;    # Display user message&#10;    with st.chat_message(&quot;user&quot;):&#10;        st.markdown(prompt)&#10;&#10;    # Add user message to history&#10;    st.session_state.chat_history.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt})&#10;&#10;    # Generate AI response with RAG&#10;    with st.chat_message(&quot;assistant&quot;):&#10;        try:&#10;            # Get relevant context from vector store (RAG)&#10;            context, sources = get_relevant_context(prompt, top_k=3)&#10;&#10;            # Create enhanced prompt with context&#10;            if context:&#10;                enhanced_prompt = f&quot;&quot;&quot;Based on the following context from the documentation, please answer the user's question.&#10;&#10;Context:&#10;{context}&#10;&#10;User Question: {prompt}&#10;&#10;Please provide a clear and helpful answer based on the context above. If the context doesn't contain relevant information, say so and provide a general answer.&quot;&quot;&quot;&#10;            else:&#10;                enhanced_prompt = prompt&#10;&#10;            # Create chat context from history&#10;            chat = model.start_chat(history=[])&#10;&#10;            # Add previous messages for context (excluding the current one)&#10;            for msg in st.session_state.chat_history[:-1]:&#10;                if msg[&quot;role&quot;] == &quot;user&quot;:&#10;                    chat.send_message(msg[&quot;content&quot;])&#10;&#10;            # Send current message and get streaming response&#10;            response = chat.send_message(enhanced_prompt, stream=True)&#10;&#10;            # Display streaming response&#10;            message_placeholder = st.empty()&#10;            full_response = &quot;&quot;&#10;&#10;            for chunk in response:&#10;                if chunk.text:&#10;                    full_response += chunk.text&#10;                    message_placeholder.markdown(full_response + &quot;▌&quot;)&#10;&#10;            # Remove cursor and show final response&#10;            message_placeholder.markdown(full_response)&#10;&#10;            # Show sources if available&#10;            if sources:&#10;                with st.expander(&quot; Sources&quot;):&#10;                    for i, source in enumerate(sources, 1):&#10;                        st.markdown(f&quot;**Source {i}:** (Similarity: {source['similarity']:.2%})&quot;)&#10;                        st.text(f&quot; Document: {source['metadata'].get('source_id', 'Unknown')}&quot;)&#10;                        st.text(f&quot; Chunk: {source['metadata'].get('chunk_index', 'N/A')}&quot;)&#10;                        st.markdown(f&quot;```\n{source['text'][:300]}...\n```&quot;)&#10;&#10;        except Exception as e:&#10;            full_response = f&quot;❌ Error: {str(e)}&quot;&#10;            st.error(full_response)&#10;            sources = []&#10;&#10;    # Add assistant response to history&#10;    history_entry = {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: full_response}&#10;    if sources:&#10;        history_entry[&quot;sources&quot;] = sources&#10;&#10;    st.session_state.chat_history.append(history_entry)&#10;&#10;    # Trim history to keep it manageable&#10;    trim_history()&#10;" />
              <option name="updatedContent" value="import streamlit as st&#10;import os&#10;from dotenv import load_dotenv&#10;import google.generativeai as genai&#10;from pathlib import Path&#10;import tempfile&#10;&#10;# Import project modules&#10;from src.data_processing.parser import parse_document&#10;from src.data_processing.chunker import TokenChunker&#10;from src.core.embeddings import configure_gemini as configure_gemini_embeddings, embed_chunks, embed_text&#10;from src.core.vector_store import VectorStore&#10;&#10;# Load environment variables&#10;load_dotenv(&quot;.env.local&quot;)&#10;&#10;# ---- Streamlit Setup ---- #&#10;st.set_page_config(layout=&quot;wide&quot;, page_title=&quot;AI Documentation Assistant&quot;)&#10;st.title(&quot; AI Documentation Assistant&quot;)&#10;&#10;# ---- Configure Gemini ---- #&#10;@st.cache_resource&#10;def configure_gemini():&#10;    api_key = os.environ.get(&quot;GOOGLE_API_KEY&quot;)&#10;    if not api_key:&#10;        st.error(&quot;⚠️ GOOGLE_API_KEY environment variable not set. Please add it to your .env.local file.&quot;)&#10;        st.stop()&#10;    genai.configure(api_key=api_key)&#10;    configure_gemini_embeddings(api_key)  # Also configure for embeddings&#10;    return genai.GenerativeModel('gemini-2.5-pro')&#10;&#10;model = configure_gemini()&#10;&#10;# ---- Initialize Vector Store (Step 3.2) ---- #&#10;@st.cache_resource&#10;def get_vector_store():&#10;    &quot;&quot;&quot;&#10;    Initialize ChromaDB with:&#10;    - Vector dimension: 768 (Gemini text-embedding-004)&#10;    - Distance metric: Cosine similarity&#10;    - Storage: Chroma Cloud&#10;    &quot;&quot;&quot;&#10;    return VectorStore(&#10;        collection_name=&quot;embeddings&quot;,  # Your Chroma Cloud collection&#10;        use_cloud=True  # Use Chroma Cloud instead of local storage&#10;    )&#10;&#10;vector_store = get_vector_store()&#10;&#10;# ---- Sidebar Settings ---- #&#10;st.sidebar.header(&quot;⚙️ Settings&quot;)&#10;MAX_HISTORY = st.sidebar.number_input(&#10;    &quot;Max History Messages&quot;,&#10;    min_value=2,&#10;    max_value=20,&#10;    value=10,&#10;    step=2,&#10;    help=&quot;Number of messages to keep in chat history&quot;&#10;)&#10;&#10;st.sidebar.divider()&#10;&#10;# ---- Document Upload Section ---- #&#10;st.sidebar.header(&quot; Document Upload&quot;)&#10;uploaded_file = st.sidebar.file_uploader(&#10;    &quot;Upload documentation (PDF or HTML)&quot;,&#10;    type=[&quot;pdf&quot;, &quot;html&quot;],&#10;    help=&quot;Upload a document to add to the knowledge base&quot;&#10;)&#10;&#10;if uploaded_file is not None:&#10;    if st.sidebar.button(&quot; Process Document&quot;):&#10;        with st.sidebar.status(&quot;Processing document...&quot;, expanded=True) as status:&#10;            try:&#10;                # Save uploaded file temporarily&#10;                with tempfile.NamedTemporaryFile(delete=False, suffix=Path(uploaded_file.name).suffix) as tmp_file:&#10;                    tmp_file.write(uploaded_file.getvalue())&#10;                    tmp_path = tmp_file.name&#10;&#10;                st.write(&quot; Parsing document...&quot;)&#10;                # Step 1: Parse document&#10;                text = parse_document(tmp_path)&#10;&#10;                st.write(&quot;✂️ Chunking text...&quot;)&#10;                # Step 2: Chunk the text with metadata&#10;                chunker = TokenChunker(strategy=&quot;tokens&quot;)&#10;                chunks = chunker.chunk(&#10;                    text=text,&#10;                    chunk_size=512,&#10;                    chunk_overlap=50,&#10;                    source_id=uploaded_file.name,&#10;                    source_path=uploaded_file.name&#10;                )&#10;&#10;                st.write(f&quot;Created {len(chunks)} chunks&quot;)&#10;&#10;                st.write(&quot; Generating embeddings...&quot;)&#10;                # Step 3: Generate embeddings (768-dimensional vectors)&#10;                embeddings = embed_chunks(chunks)&#10;&#10;                st.write(&quot; Storing in vector database...&quot;)&#10;                # Step 3.3: Store in ChromaDB with cosine similarity&#10;                ids = vector_store.add_chunks(chunks, embeddings)&#10;&#10;                # Clean up temporary file&#10;                os.unlink(tmp_path)&#10;&#10;                status.update(label=f&quot;✅ Successfully processed {uploaded_file.name}!&quot;, state=&quot;complete&quot;)&#10;                st.sidebar.success(f&quot;Added {len(chunks)} chunks to knowledge base!&quot;)&#10;&#10;            except Exception as e:&#10;                status.update(label=&quot;❌ Error processing document&quot;, state=&quot;error&quot;)&#10;                st.sidebar.error(f&quot;Error: {str(e)}&quot;)&#10;&#10;st.sidebar.divider()&#10;&#10;# ---- Database Statistics ---- #&#10;st.sidebar.header(&quot; Database Stats&quot;)&#10;try:&#10;    total_chunks = vector_store.count()&#10;    st.sidebar.metric(&quot;Total Chunks&quot;, total_chunks)&#10;&#10;    if total_chunks &gt; 0:&#10;        # Get all items to show unique sources&#10;        all_items = vector_store.collection.get()&#10;        if all_items and all_items[&quot;metadatas&quot;]:&#10;            unique_sources = set(&#10;                meta.get(&quot;source_id&quot;, &quot;unknown&quot;)&#10;                for meta in all_items[&quot;metadatas&quot;]&#10;            )&#10;            st.sidebar.metric(&quot;Documents&quot;, len(unique_sources))&#10;&#10;            with st.sidebar.expander(&quot; View Sources&quot;):&#10;                for source in unique_sources:&#10;                    chunks_in_source = sum(&#10;                        1 for meta in all_items[&quot;metadatas&quot;]&#10;                        if meta.get(&quot;source_id&quot;) == source&#10;                    )&#10;                    st.write(f&quot;• {source} ({chunks_in_source} chunks)&quot;)&#10;    else:&#10;        st.sidebar.info(&quot; Upload a document to get started&quot;)&#10;&#10;except Exception as e:&#10;    st.sidebar.error(f&quot;Error reading stats: {str(e)}&quot;)&#10;&#10;st.sidebar.divider()&#10;&#10;if st.sidebar.button(&quot;️ Clear Chat History&quot;):&#10;    st.session_state.chat_history = []&#10;    st.rerun()&#10;&#10;# ---- Initialize Chat History ---- #&#10;if &quot;chat_history&quot; not in st.session_state:&#10;    st.session_state.chat_history = []&#10;&#10;# ---- Display Chat History ---- #&#10;for msg in st.session_state.chat_history:&#10;    with st.chat_message(msg[&quot;role&quot;]):&#10;        st.markdown(msg[&quot;content&quot;])&#10;        # Show sources if available&#10;        if msg[&quot;role&quot;] == &quot;assistant&quot; and &quot;sources&quot; in msg:&#10;            with st.expander(&quot; Sources&quot;):&#10;                for i, source in enumerate(msg[&quot;sources&quot;], 1):&#10;                    st.markdown(f&quot;**Source {i}:** (Similarity: {source['similarity']:.2%})&quot;)&#10;                    st.text(f&quot; Document: {source['metadata'].get('source_id', 'Unknown')}&quot;)&#10;                    st.text(f&quot; Chunk: {source['metadata'].get('chunk_index', 'N/A')}&quot;)&#10;                    st.markdown(f&quot;```\n{source['text'][:300]}...\n```&quot;)&#10;&#10;# ---- Trim Function (Removes Oldest Messages) ---- #&#10;def trim_history():&#10;    &quot;&quot;&quot;Keep only the most recent MAX_HISTORY messages&quot;&quot;&quot;&#10;    if len(st.session_state.chat_history) &gt; MAX_HISTORY:&#10;        st.session_state.chat_history = st.session_state.chat_history[-MAX_HISTORY:]&#10;&#10;# ---- RAG Function ---- #&#10;def get_relevant_context(query: str, top_k: int = 3) -&gt; tuple:&#10;    &quot;&quot;&quot;&#10;    Retrieve relevant context from vector store using RAG.&#10;    Uses cosine similarity to find most relevant chunks.&#10;&#10;    Returns:&#10;        tuple: (context_text, sources_list)&#10;    &quot;&quot;&quot;&#10;    try:&#10;        # Generate embedding for the query (768-dimensional)&#10;        query_embedding = embed_text(query, task_type=&quot;retrieval_query&quot;)&#10;&#10;        # Search vector store using cosine similarity&#10;        results = vector_store.search(query_embedding, top_k=top_k)&#10;&#10;        if not results:&#10;            return &quot;&quot;, []&#10;&#10;        # Format context&#10;        context_parts = []&#10;        sources = []&#10;&#10;        for i, (text, similarity, metadata) in enumerate(results, 1):&#10;            context_parts.append(f&quot;[Context {i}]:\n{text}\n&quot;)&#10;            sources.append({&#10;                &quot;text&quot;: text,&#10;                &quot;similarity&quot;: similarity,&#10;                &quot;metadata&quot;: metadata&#10;            })&#10;&#10;        context = &quot;\n&quot;.join(context_parts)&#10;        return context, sources&#10;&#10;    except Exception as e:&#10;        st.error(f&quot;Error retrieving context: {str(e)}&quot;)&#10;        return &quot;&quot;, []&#10;&#10;# ---- Handle User Input ---- #&#10;if prompt := st.chat_input(&quot;Ask me anything about your documentation...&quot;):&#10;    # Display user message&#10;    with st.chat_message(&quot;user&quot;):&#10;        st.markdown(prompt)&#10;&#10;    # Add user message to history&#10;    st.session_state.chat_history.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt})&#10;&#10;    # Generate AI response with RAG&#10;    with st.chat_message(&quot;assistant&quot;):&#10;        try:&#10;            # Get relevant context from vector store (RAG)&#10;            context, sources = get_relevant_context(prompt, top_k=3)&#10;&#10;            # Create enhanced prompt with context&#10;            if context:&#10;                enhanced_prompt = f&quot;&quot;&quot;Based on the following context from the documentation, please answer the user's question.&#10;&#10;Context:&#10;{context}&#10;&#10;User Question: {prompt}&#10;&#10;Please provide a clear and helpful answer based on the context above. If the context doesn't contain relevant information, say so and provide a general answer.&quot;&quot;&quot;&#10;            else:&#10;                enhanced_prompt = prompt&#10;&#10;            # Create chat context from history&#10;            chat = model.start_chat(history=[])&#10;&#10;            # Add previous messages for context (excluding the current one)&#10;            for msg in st.session_state.chat_history[:-1]:&#10;                if msg[&quot;role&quot;] == &quot;user&quot;:&#10;                    chat.send_message(msg[&quot;content&quot;])&#10;&#10;            # Send current message and get streaming response&#10;            response = chat.send_message(enhanced_prompt, stream=True)&#10;&#10;            # Display streaming response&#10;            message_placeholder = st.empty()&#10;            full_response = &quot;&quot;&#10;&#10;            for chunk in response:&#10;                if chunk.text:&#10;                    full_response += chunk.text&#10;                    message_placeholder.markdown(full_response + &quot;▌&quot;)&#10;&#10;            # Remove cursor and show final response&#10;            message_placeholder.markdown(full_response)&#10;&#10;            # Show sources if available&#10;            if sources:&#10;                with st.expander(&quot; Sources&quot;):&#10;                    for i, source in enumerate(sources, 1):&#10;                        st.markdown(f&quot;**Source {i}:** (Similarity: {source['similarity']:.2%})&quot;)&#10;                        st.text(f&quot; Document: {source['metadata'].get('source_id', 'Unknown')}&quot;)&#10;                        st.text(f&quot; Chunk: {source['metadata'].get('chunk_index', 'N/A')}&quot;)&#10;                        st.markdown(f&quot;```\n{source['text'][:300]}...\n```&quot;)&#10;&#10;        except Exception as e:&#10;            full_response = f&quot;❌ Error: {str(e)}&quot;&#10;            st.error(full_response)&#10;            sources = []&#10;&#10;    # Add assistant response to history&#10;    history_entry = {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: full_response}&#10;    if sources:&#10;        history_entry[&quot;sources&quot;] = sources&#10;&#10;    st.session_state.chat_history.append(history_entry)&#10;&#10;    # Trim history to keep it manageable&#10;    trim_history()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/src/core/embeddings.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/src/core/embeddings.py" />
              <option name="originalContent" value="import os&#10;from typing import List, Optional, Union&#10;from dotenv import load_dotenv&#10;import google.generativeai as genai&#10;&#10;# Import Chunk model using relative import&#10;import sys&#10;from pathlib import Path&#10;# Add parent directory to path to enable imports&#10;sys.path.insert(0, str(Path(__file__).resolve().parent.parent))&#10;&#10;from data_processing.models import Chunk&#10;&#10;&#10;def configure_gemini(api_key: Optional[str] = None) -&gt; None:&#10;    load_dotenv(&quot;../.env.local&quot;)&#10;    if api_key is None:&#10;        api_key = os.environ.get(&quot;GOOGLE_API_KEY&quot;)&#10;    if not api_key:&#10;        raise Exception(&quot;GOOGLE_API_KEY environment variable not set&quot;)&#10;    genai.configure(api_key=api_key)&#10;&#10;&#10;def embed_text(text: str,&#10;               model: str = &quot;models/text-embedding-004&quot;,&#10;               task_type: str = &quot;retrieval_document&quot;) -&gt; List[float]:&#10;    &quot;&quot;&quot;Low-level function to embed a single text string.&quot;&quot;&quot;&#10;    result = genai.embed_content(model=model, content=text, task_type=task_type)&#10;    return result[&quot;embedding&quot;]&#10;&#10;&#10;def embed_chunks(&#10;    chunks: Union[Chunk, List[Chunk]],&#10;    model: str = &quot;models/text-embedding-004&quot;,&#10;    task_type: str = &quot;retrieval_document&quot;&#10;) -&gt; Union[List[float], List[List[float]]]:&#10;    &quot;&quot;&quot;&#10;    Service wrapper for embedding model (Step 2.2).&#10;    Accepts a Chunk object or list of Chunk objects and returns embedding vector(s).&#10;&#10;    Args:&#10;        chunks: Single Chunk or list of Chunk objects&#10;        model: Gemini embedding model name&#10;        task_type: Task type for embeddings (retrieval_document or retrieval_query)&#10;&#10;    Returns:&#10;        Single embedding vector (List[float]) if input is single Chunk,&#10;        or list of embedding vectors (List[List[float]]) if input is list of Chunks&#10;&#10;    Example:&#10;        # Single chunk&#10;        chunk = Chunk(text=&quot;Hello world&quot;, metadata={&quot;source&quot;: &quot;doc1&quot;})&#10;        vec = embed_chunks(chunk)&#10;&#10;        # Multiple chunks&#10;        chunks = [Chunk(text=&quot;chunk 1&quot;, metadata={}), Chunk(text=&quot;chunk 2&quot;, metadata={})]&#10;        vecs = embed_chunks(chunks)&#10;    &quot;&quot;&quot;&#10;    # Handle single Chunk&#10;    if isinstance(chunks, Chunk):&#10;        return embed_text(chunks.text, model=model, task_type=task_type)&#10;&#10;    # Handle list of Chunks&#10;    elif isinstance(chunks, list):&#10;        embeddings = []&#10;        for chunk in chunks:&#10;            if not isinstance(chunk, Chunk):&#10;                raise TypeError(f&quot;Expected Chunk object, got {type(chunk)}&quot;)&#10;            embedding = embed_text(chunk.text, model=model, task_type=task_type)&#10;            embeddings.append(embedding)&#10;        return embeddings&#10;&#10;    else:&#10;        raise TypeError(f&quot;chunks must be Chunk or List[Chunk], got {type(chunks)}&quot;)&#10;" />
              <option name="updatedContent" value="import os&#10;from typing import List, Optional, Union&#10;from dotenv import load_dotenv&#10;import google.generativeai as genai&#10;&#10;from src.data_processing.models import Chunk&#10;&#10;&#10;def configure_gemini(api_key: Optional[str] = None) -&gt; None:&#10;    load_dotenv(&quot;../.env.local&quot;)&#10;    if api_key is None:&#10;        api_key = os.environ.get(&quot;GOOGLE_API_KEY&quot;)&#10;    if not api_key:&#10;        raise Exception(&quot;GOOGLE_API_KEY environment variable not set&quot;)&#10;    genai.configure(api_key=api_key)&#10;&#10;&#10;def embed_text(text: str,&#10;               model: str = &quot;models/text-embedding-004&quot;,&#10;               task_type: str = &quot;retrieval_document&quot;) -&gt; List[float]:&#10;    &quot;&quot;&quot;Low-level function to embed a single text string.&quot;&quot;&quot;&#10;    result = genai.embed_content(model=model, content=text, task_type=task_type)&#10;    return result[&quot;embedding&quot;]&#10;&#10;&#10;def embed_chunks(&#10;    chunks: Union[Chunk, List[Chunk]],&#10;    model: str = &quot;models/text-embedding-004&quot;,&#10;    task_type: str = &quot;retrieval_document&quot;&#10;) -&gt; Union[List[float], List[List[float]]]:&#10;    &quot;&quot;&quot;&#10;    Service wrapper for embedding model (Step 2.2).&#10;    Accepts a Chunk object or list of Chunk objects and returns embedding vector(s).&#10;&#10;    Args:&#10;        chunks: Single Chunk or list of Chunk objects&#10;        model: Gemini embedding model name&#10;        task_type: Task type for embeddings (retrieval_document or retrieval_query)&#10;&#10;    Returns:&#10;        Single embedding vector (List[float]) if input is single Chunk,&#10;        or list of embedding vectors (List[List[float]]) if input is list of Chunks&#10;&#10;    Example:&#10;        # Single chunk&#10;        chunk = Chunk(text=&quot;Hello world&quot;, metadata={&quot;source&quot;: &quot;doc1&quot;})&#10;        vec = embed_chunks(chunk)&#10;&#10;        # Multiple chunks&#10;        chunks = [Chunk(text=&quot;chunk 1&quot;, metadata={}), Chunk(text=&quot;chunk 2&quot;, metadata={})]&#10;        vecs = embed_chunks(chunks)&#10;    &quot;&quot;&quot;&#10;    # Handle single Chunk&#10;    if isinstance(chunks, Chunk):&#10;        return embed_text(chunks.text, model=model, task_type=task_type)&#10;&#10;    # Handle list of Chunks&#10;    elif isinstance(chunks, list):&#10;        embeddings = []&#10;        for chunk in chunks:&#10;            if not isinstance(chunk, Chunk):&#10;                raise TypeError(f&quot;Expected Chunk object, got {type(chunk)}&quot;)&#10;            embedding = embed_text(chunk.text, model=model, task_type=task_type)&#10;            embeddings.append(embedding)&#10;        return embeddings&#10;&#10;    else:&#10;        raise TypeError(f&quot;chunks must be Chunk or List[Chunk], got {type(chunks)}&quot;)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/src/core/vector_store.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/src/core/vector_store.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Vector store implementation using ChromaDB (Step 3.2 &amp; 3.3)&#10;Handles storage and retrieval of document embeddings with metadata.&#10;&quot;&quot;&quot;&#10;import uuid&#10;from typing import List, Dict, Any, Optional, Tuple&#10;import chromadb&#10;from chromadb.config import Settings&#10;&#10;# Import project models using relative path&#10;import sys&#10;from pathlib import Path&#10;# Add parent directory to path to enable imports&#10;sys.path.insert(0, str(Path(__file__).resolve().parent.parent))&#10;&#10;from data_processing.models import Chunk&#10;&#10;&#10;class VectorStore:&#10;    &quot;&quot;&quot;&#10;    Service for storing and retrieving document embeddings in ChromaDB.&#10;&#10;    Configuration (Step 3.2):&#10;    - Vector dimension: 768 (Gemini text-embedding-004)&#10;    - Distance metric: Cosine similarity&#10;    - Storage: Persistent local database&#10;&#10;    Storage logic (Step 3.3):&#10;    - Each record contains: unique ID, embedding vector, metadata&#10;    - Metadata includes: source document, chunk index, original text&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, collection_name: str = &quot;documents&quot;, persist_directory: str = &quot;./chroma_db&quot;):&#10;        &quot;&quot;&quot;&#10;        Initialize ChromaDB vector store.&#10;&#10;        Args:&#10;            collection_name: Name of the collection to store embeddings&#10;            persist_directory: Directory to persist the database&#10;        &quot;&quot;&quot;&#10;        # Step 3.2: Configure ChromaDB with persistence&#10;        self.client = chromadb.PersistentClient(&#10;            path=persist_directory,&#10;            settings=Settings(&#10;                anonymized_telemetry=False,&#10;                allow_reset=True&#10;            )&#10;        )&#10;&#10;        # Create or get collection with cosine similarity metric&#10;        # ChromaDB uses L2 by default, but we can normalize vectors for cosine similarity&#10;        self.collection = self.client.get_or_create_collection(&#10;            name=collection_name,&#10;            metadata={&#10;                &quot;hnsw:space&quot;: &quot;cosine&quot;,  # Use cosine similarity&#10;                &quot;description&quot;: &quot;Document chunks with embeddings from Gemini text-embedding-004&quot;,&#10;                &quot;embedding_dimension&quot;: 768  # Gemini text-embedding-004 dimension&#10;            }&#10;        )&#10;&#10;    def add_chunks(&#10;        self,&#10;        chunks: List[Chunk],&#10;        embeddings: List[List[float]],&#10;        ids: Optional[List[str]] = None&#10;    ) -&gt; List[str]:&#10;        &quot;&quot;&quot;&#10;        Step 3.3: Store chunks with their embeddings and metadata.&#10;&#10;        Args:&#10;            chunks: List of Chunk objects with text and metadata&#10;            embeddings: List of embedding vectors (must match chunks length)&#10;            ids: Optional list of unique IDs (auto-generated if not provided)&#10;&#10;        Returns:&#10;            List of IDs for the stored chunks&#10;&#10;        Example:&#10;            chunks = [Chunk(text=&quot;Hello&quot;, metadata={&quot;page&quot;: 1})]&#10;            embeddings = [[0.1, 0.2, ...]]  # 768-dim vector&#10;            ids = store.add_chunks(chunks, embeddings)&#10;        &quot;&quot;&quot;&#10;        if len(chunks) != len(embeddings):&#10;            raise ValueError(f&quot;Number of chunks ({len(chunks)}) must match embeddings ({len(embeddings)})&quot;)&#10;&#10;        # Generate unique IDs if not provided&#10;        if ids is None:&#10;            ids = [str(uuid.uuid4()) for _ in chunks]&#10;&#10;        # Prepare data for ChromaDB&#10;        documents = []  # Original text for each chunk&#10;        metadatas = []  # Metadata for each chunk&#10;&#10;        for chunk in chunks:&#10;            documents.append(chunk.text)&#10;&#10;            # Flatten metadata for ChromaDB (must be JSON-serializable)&#10;            meta = {&#10;                &quot;source_id&quot;: str(chunk.metadata.get(&quot;source_id&quot;, &quot;unknown&quot;)),&#10;                &quot;source_path&quot;: str(chunk.metadata.get(&quot;source_path&quot;, &quot;&quot;)),&#10;                &quot;chunk_index&quot;: int(chunk.metadata.get(&quot;chunk_index&quot;, 0)),&#10;                &quot;strategy&quot;: str(chunk.metadata.get(&quot;strategy&quot;, &quot;unknown&quot;)),&#10;            }&#10;&#10;            # Add optional fields if present&#10;            if &quot;page_number&quot; in chunk.metadata:&#10;                meta[&quot;page_number&quot;] = int(chunk.metadata[&quot;page_number&quot;])&#10;            if &quot;chapter_title&quot; in chunk.metadata:&#10;                meta[&quot;chapter_title&quot;] = str(chunk.metadata[&quot;chapter_title&quot;])&#10;&#10;            metadatas.append(meta)&#10;&#10;        # Step 3.3: Store in vector database&#10;        self.collection.add(&#10;            ids=ids,&#10;            embeddings=embeddings,&#10;            documents=documents,&#10;            metadatas=metadatas&#10;        )&#10;&#10;        return ids&#10;&#10;    def search(&#10;        self,&#10;        query_embedding: List[float],&#10;        top_k: int = 3,&#10;        filter_metadata: Optional[Dict[str, Any]] = None&#10;    ) -&gt; List[Tuple[str, float, Dict[str, Any]]]:&#10;        &quot;&quot;&quot;&#10;        Search for similar chunks using cosine similarity.&#10;&#10;        Args:&#10;            query_embedding: Query vector (768-dim for Gemini)&#10;            top_k: Number of results to return&#10;            filter_metadata: Optional metadata filters (e.g., {&quot;source_id&quot;: &quot;doc1.pdf&quot;})&#10;&#10;        Returns:&#10;            List of tuples: (chunk_text, similarity_score, metadata)&#10;        &quot;&quot;&quot;&#10;        results = self.collection.query(&#10;            query_embeddings=[query_embedding],&#10;            n_results=top_k,&#10;            where=filter_metadata&#10;        )&#10;&#10;        # Format results&#10;        output = []&#10;        if results[&quot;documents&quot;] and results[&quot;documents&quot;][0]:&#10;            for i in range(len(results[&quot;documents&quot;][0])):&#10;                text = results[&quot;documents&quot;][0][i]&#10;                distance = results[&quot;distances&quot;][0][i] if results[&quot;distances&quot;] else 0.0&#10;                # Convert distance to similarity (ChromaDB returns distances)&#10;                similarity = 1.0 - distance  # For cosine, distance = 1 - similarity&#10;                metadata = results[&quot;metadatas&quot;][0][i] if results[&quot;metadatas&quot;] else {}&#10;                output.append((text, similarity, metadata))&#10;&#10;        return output&#10;&#10;    def get_by_source(self, source_id: str) -&gt; List[Dict[str, Any]]:&#10;        &quot;&quot;&quot;&#10;        Retrieve all chunks from a specific source document.&#10;&#10;        Args:&#10;            source_id: The source document ID&#10;&#10;        Returns:&#10;            List of chunk data with metadata&#10;        &quot;&quot;&quot;&#10;        results = self.collection.get(&#10;            where={&quot;source_id&quot;: source_id}&#10;        )&#10;&#10;        output = []&#10;        if results[&quot;documents&quot;]:&#10;            for i in range(len(results[&quot;documents&quot;])):&#10;                output.append({&#10;                    &quot;id&quot;: results[&quot;ids&quot;][i],&#10;                    &quot;text&quot;: results[&quot;documents&quot;][i],&#10;                    &quot;metadata&quot;: results[&quot;metadatas&quot;][i] if results[&quot;metadatas&quot;] else {}&#10;                })&#10;&#10;        return output&#10;&#10;    def delete_by_source(self, source_id: str) -&gt; int:&#10;        &quot;&quot;&quot;&#10;        Delete all chunks from a specific source document.&#10;&#10;        Args:&#10;            source_id: The source document ID&#10;&#10;        Returns:&#10;            Number of chunks deleted&#10;        &quot;&quot;&quot;&#10;        # Get IDs to delete&#10;        results = self.collection.get(&#10;            where={&quot;source_id&quot;: source_id}&#10;        )&#10;&#10;        if results[&quot;ids&quot;]:&#10;            self.collection.delete(ids=results[&quot;ids&quot;])&#10;            return len(results[&quot;ids&quot;])&#10;&#10;        return 0&#10;&#10;    def count(self) -&gt; int:&#10;        &quot;&quot;&quot;Get total number of chunks in the store.&quot;&quot;&quot;&#10;        return self.collection.count()&#10;&#10;    def clear(self):&#10;        &quot;&quot;&quot;Delete all data from the collection.&quot;&quot;&quot;&#10;        self.client.delete_collection(self.collection.name)&#10;        # Recreate empty collection&#10;        self.collection = self.client.get_or_create_collection(&#10;            name=self.collection.name,&#10;            metadata={&#10;                &quot;hnsw:space&quot;: &quot;cosine&quot;,&#10;                &quot;description&quot;: &quot;Document chunks with embeddings from Gemini text-embedding-004&quot;,&#10;                &quot;embedding_dimension&quot;: 768&#10;            }&#10;        )&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Vector store implementation using ChromaDB (Step 3.2 &amp; 3.3)&#10;Handles storage and retrieval of document embeddings with metadata.&#10;&quot;&quot;&quot;&#10;import os&#10;import uuid&#10;from typing import List, Dict, Any, Optional, Tuple&#10;import chromadb&#10;from chromadb.config import Settings&#10;&#10;from src.data_processing.models import Chunk&#10;&#10;&#10;class VectorStore:&#10;    &quot;&quot;&quot;&#10;    Service for storing and retrieving document embeddings in ChromaDB.&#10;&#10;    Configuration (Step 3.2):&#10;    - Vector dimension: 768 (Gemini text-embedding-004)&#10;    - Distance metric: Cosine similarity&#10;    - Storage: Chroma Cloud (remote) or local persistent database&#10;&#10;    Storage logic (Step 3.3):&#10;    - Each record contains: unique ID, embedding vector, metadata&#10;    - Metadata includes: source document, chunk index, original text&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(&#10;        self, &#10;        collection_name: str = &quot;embeddings&quot;, &#10;        persist_directory: str = &quot;./chroma_db&quot;,&#10;        use_cloud: bool = True&#10;    ):&#10;        &quot;&quot;&quot;&#10;        Initialize ChromaDB vector store.&#10;&#10;        Args:&#10;            collection_name: Name of the collection to store embeddings&#10;            persist_directory: Directory to persist the database (local mode)&#10;            use_cloud: Whether to use Chroma Cloud (True) or local storage (False)&#10;        &quot;&quot;&quot;&#10;        # Step 3.2: Configure ChromaDB with cloud or local persistence&#10;        if use_cloud:&#10;            # Use Chroma Cloud&#10;            chroma_api_key = os.environ.get(&quot;CHROMA_API_KEY&quot;)&#10;            tenant = os.environ.get(&quot;TENANT&quot;)&#10;            database = os.environ.get(&quot;DATABASE&quot;)&#10;            &#10;            if not all([chroma_api_key, tenant, database]):&#10;                raise ValueError(&#10;                    &quot;Chroma Cloud credentials missing. Please set CHROMA_API_KEY, &quot;&#10;                    &quot;TENANT, and DATABASE in .env.local&quot;&#10;                )&#10;            &#10;            self.client = chromadb.HttpClient(&#10;                host=&quot;api.trychroma.com&quot;,&#10;                ssl=True,&#10;                headers={&#10;                    &quot;x-chroma-token&quot;: chroma_api_key&#10;                },&#10;                tenant=tenant,&#10;                database=database&#10;            )&#10;            print(f&quot;✅ Connected to Chroma Cloud - Database: {database}, Tenant: {tenant}&quot;)&#10;        else:&#10;            # Use local persistent storage&#10;            self.client = chromadb.PersistentClient(&#10;                path=persist_directory,&#10;                settings=Settings(&#10;                    anonymized_telemetry=False,&#10;                    allow_reset=True&#10;                )&#10;            )&#10;            print(f&quot;✅ Connected to local ChromaDB at: {persist_directory}&quot;)&#10;&#10;        # Create or get collection with cosine similarity metric&#10;        self.collection = self.client.get_or_create_collection(&#10;            name=collection_name,&#10;            metadata={&#10;                &quot;hnsw:space&quot;: &quot;cosine&quot;,  # Use cosine similarity&#10;                &quot;description&quot;: &quot;Document chunks with embeddings from Gemini text-embedding-004&quot;,&#10;                &quot;embedding_dimension&quot;: 768  # Gemini text-embedding-004 dimension&#10;            }&#10;        )&#10;        print(f&quot;✅ Using collection: {collection_name}&quot;)&#10;&#10;    def add_chunks(&#10;        self,&#10;        chunks: List[Chunk],&#10;        embeddings: List[List[float]],&#10;        ids: Optional[List[str]] = None&#10;    ) -&gt; List[str]:&#10;        &quot;&quot;&quot;&#10;        Step 3.3: Store chunks with their embeddings and metadata.&#10;&#10;        Args:&#10;            chunks: List of Chunk objects with text and metadata&#10;            embeddings: List of embedding vectors (must match chunks length)&#10;            ids: Optional list of unique IDs (auto-generated if not provided)&#10;&#10;        Returns:&#10;            List of IDs for the stored chunks&#10;&#10;        Example:&#10;            chunks = [Chunk(text=&quot;Hello&quot;, metadata={&quot;page&quot;: 1})]&#10;            embeddings = [[0.1, 0.2, ...]]  # 768-dim vector&#10;            ids = store.add_chunks(chunks, embeddings)&#10;        &quot;&quot;&quot;&#10;        if len(chunks) != len(embeddings):&#10;            raise ValueError(f&quot;Number of chunks ({len(chunks)}) must match embeddings ({len(embeddings)})&quot;)&#10;&#10;        # Generate unique IDs if not provided&#10;        if ids is None:&#10;            ids = [str(uuid.uuid4()) for _ in chunks]&#10;&#10;        # Prepare data for ChromaDB&#10;        documents = []  # Original text for each chunk&#10;        metadatas = []  # Metadata for each chunk&#10;&#10;        for chunk in chunks:&#10;            documents.append(chunk.text)&#10;&#10;            # Flatten metadata for ChromaDB (must be JSON-serializable)&#10;            meta = {&#10;                &quot;source_id&quot;: str(chunk.metadata.get(&quot;source_id&quot;, &quot;unknown&quot;)),&#10;                &quot;source_path&quot;: str(chunk.metadata.get(&quot;source_path&quot;, &quot;&quot;)),&#10;                &quot;chunk_index&quot;: int(chunk.metadata.get(&quot;chunk_index&quot;, 0)),&#10;                &quot;strategy&quot;: str(chunk.metadata.get(&quot;strategy&quot;, &quot;unknown&quot;)),&#10;            }&#10;&#10;            # Add optional fields if present&#10;            if &quot;page_number&quot; in chunk.metadata:&#10;                meta[&quot;page_number&quot;] = int(chunk.metadata[&quot;page_number&quot;])&#10;            if &quot;chapter_title&quot; in chunk.metadata:&#10;                meta[&quot;chapter_title&quot;] = str(chunk.metadata[&quot;chapter_title&quot;])&#10;&#10;            metadatas.append(meta)&#10;&#10;        # Step 3.3: Store in vector database&#10;        self.collection.add(&#10;            ids=ids,&#10;            embeddings=embeddings,&#10;            documents=documents,&#10;            metadatas=metadatas&#10;        )&#10;&#10;        return ids&#10;&#10;    def search(&#10;        self,&#10;        query_embedding: List[float],&#10;        top_k: int = 3,&#10;        filter_metadata: Optional[Dict[str, Any]] = None&#10;    ) -&gt; List[Tuple[str, float, Dict[str, Any]]]:&#10;        &quot;&quot;&quot;&#10;        Search for similar chunks using cosine similarity.&#10;&#10;        Args:&#10;            query_embedding: Query vector (768-dim for Gemini)&#10;            top_k: Number of results to return&#10;            filter_metadata: Optional metadata filters (e.g., {&quot;source_id&quot;: &quot;doc1.pdf&quot;})&#10;&#10;        Returns:&#10;            List of tuples: (chunk_text, similarity_score, metadata)&#10;        &quot;&quot;&quot;&#10;        results = self.collection.query(&#10;            query_embeddings=[query_embedding],&#10;            n_results=top_k,&#10;            where=filter_metadata&#10;        )&#10;&#10;        # Format results&#10;        output = []&#10;        if results[&quot;documents&quot;] and results[&quot;documents&quot;][0]:&#10;            for i in range(len(results[&quot;documents&quot;][0])):&#10;                text = results[&quot;documents&quot;][0][i]&#10;                distance = results[&quot;distances&quot;][0][i] if results[&quot;distances&quot;] else 0.0&#10;                # Convert distance to similarity (ChromaDB returns distances)&#10;                similarity = 1.0 - distance  # For cosine, distance = 1 - similarity&#10;                metadata = results[&quot;metadatas&quot;][0][i] if results[&quot;metadatas&quot;] else {}&#10;                output.append((text, similarity, metadata))&#10;&#10;        return output&#10;&#10;    def get_by_source(self, source_id: str) -&gt; List[Dict[str, Any]]:&#10;        &quot;&quot;&quot;&#10;        Retrieve all chunks from a specific source document.&#10;&#10;        Args:&#10;            source_id: The source document ID&#10;&#10;        Returns:&#10;            List of chunk data with metadata&#10;        &quot;&quot;&quot;&#10;        results = self.collection.get(&#10;            where={&quot;source_id&quot;: source_id}&#10;        )&#10;&#10;        output = []&#10;        if results[&quot;documents&quot;]:&#10;            for i in range(len(results[&quot;documents&quot;])):&#10;                output.append({&#10;                    &quot;id&quot;: results[&quot;ids&quot;][i],&#10;                    &quot;text&quot;: results[&quot;documents&quot;][i],&#10;                    &quot;metadata&quot;: results[&quot;metadatas&quot;][i] if results[&quot;metadatas&quot;] else {}&#10;                })&#10;&#10;        return output&#10;&#10;    def delete_by_source(self, source_id: str) -&gt; int:&#10;        &quot;&quot;&quot;&#10;        Delete all chunks from a specific source document.&#10;&#10;        Args:&#10;            source_id: The source document ID&#10;&#10;        Returns:&#10;            Number of chunks deleted&#10;        &quot;&quot;&quot;&#10;        # Get IDs to delete&#10;        results = self.collection.get(&#10;            where={&quot;source_id&quot;: source_id}&#10;        )&#10;&#10;        if results[&quot;ids&quot;]:&#10;            self.collection.delete(ids=results[&quot;ids&quot;])&#10;            return len(results[&quot;ids&quot;])&#10;&#10;        return 0&#10;&#10;    def count(self) -&gt; int:&#10;        &quot;&quot;&quot;Get total number of chunks in the store.&quot;&quot;&quot;&#10;        return self.collection.count()&#10;&#10;    def clear(self):&#10;        &quot;&quot;&quot;Delete all data from the collection.&quot;&quot;&quot;&#10;        self.client.delete_collection(self.collection.name)&#10;        # Recreate empty collection&#10;        self.collection = self.client.get_or_create_collection(&#10;            name=self.collection.name,&#10;            metadata={&#10;                &quot;hnsw:space&quot;: &quot;cosine&quot;,&#10;                &quot;description&quot;: &quot;Document chunks with embeddings from Gemini text-embedding-004&quot;,&#10;                &quot;embedding_dimension&quot;: 768&#10;            }&#10;        )" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/tests/test_embed_chunks.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/tests/test_embed_chunks.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Test for step 2.2: Service wrapper for embedding model&#10;Tests that embed_chunks() accepts Chunk objects and returns vectors&#10;&quot;&quot;&quot;&#10;from typing import List&#10;&#10;import pytest&#10;&#10;from data_processing.models import Chunk&#10;from core.embeddings import embed_chunks&#10;&#10;&#10;def test_embed_single_chunk(monkeypatch):&#10;    &quot;&quot;&quot;Test embedding a single Chunk object&quot;&quot;&quot;&#10;    # Mock the actual API call&#10;    def fake_embed_text(text: str, **kwargs):&#10;        return [1.0, 2.0, 3.0]&#10;&#10;    monkeypatch.setattr(&quot;core.embeddings.embed_text&quot;, fake_embed_text)&#10;&#10;    # Create a Chunk&#10;    chunk = Chunk(text=&quot;Test chunk text&quot;, metadata={&quot;source&quot;: &quot;test.pdf&quot;, &quot;page&quot;: 1})&#10;&#10;    # Embed it&#10;    result = embed_chunks(chunk)&#10;&#10;    # Should return a single vector&#10;    assert isinstance(result, list)&#10;    assert len(result) == 3&#10;    assert result == [1.0, 2.0, 3.0]&#10;&#10;&#10;def test_embed_multiple_chunks(monkeypatch):&#10;    &quot;&quot;&quot;Test embedding a list of Chunk objects&quot;&quot;&quot;&#10;    # Mock the actual API call&#10;    call_count = [0]&#10;&#10;    def fake_embed_text(text: str, **kwargs):&#10;        call_count[0] += 1&#10;        # Return different vectors based on text length&#10;        return [float(len(text)), float(call_count[0]), 0.5]&#10;&#10;    monkeypatch.setattr(&quot;core.embeddings.embed_text&quot;, fake_embed_text)&#10;&#10;    # Create multiple Chunks&#10;    chunks = [&#10;        Chunk(text=&quot;First chunk&quot;, metadata={&quot;index&quot;: 0}),&#10;        Chunk(text=&quot;Second chunk text&quot;, metadata={&quot;index&quot;: 1}),&#10;        Chunk(text=&quot;Third&quot;, metadata={&quot;index&quot;: 2}),&#10;    ]&#10;&#10;    # Embed them&#10;    result = embed_chunks(chunks)&#10;&#10;    # Should return list of vectors&#10;    assert isinstance(result, list)&#10;    assert len(result) == 3&#10;    assert all(isinstance(vec, list) for vec in result)&#10;&#10;    # Check that each chunk was processed&#10;    assert result[0] == [11.0, 1.0, 0.5]  # len(&quot;First chunk&quot;) = 11&#10;    assert result[1] == [17.0, 2.0, 0.5]  # len(&quot;Second chunk text&quot;) = 17&#10;    assert result[2] == [5.0, 3.0, 0.5]   # len(&quot;Third&quot;) = 5&#10;&#10;&#10;def test_embed_chunks_with_metadata_preserved(monkeypatch):&#10;    &quot;&quot;&quot;Verify that chunk metadata is preserved (not lost during embedding)&quot;&quot;&quot;&#10;    def fake_embed_text(text: str, **kwargs):&#10;        return [1.0, 2.0, 3.0]&#10;&#10;    monkeypatch.setattr(&quot;core.embeddings.embed_text&quot;, fake_embed_text)&#10;&#10;    # Create chunks with rich metadata&#10;    chunks = [&#10;        Chunk(&#10;            text=&quot;Python is great&quot;,&#10;            metadata={&#10;                &quot;source_id&quot;: &quot;tutorial.pdf&quot;,&#10;                &quot;page_number&quot;: 5,&#10;                &quot;chunk_index&quot;: 0,&#10;                &quot;strategy&quot;: &quot;tokens&quot;&#10;            }&#10;        ),&#10;        Chunk(&#10;            text=&quot;Machine learning is powerful&quot;,&#10;            metadata={&#10;                &quot;source_id&quot;: &quot;tutorial.pdf&quot;,&#10;                &quot;page_number&quot;: 6,&#10;                &quot;chunk_index&quot;: 1,&#10;                &quot;strategy&quot;: &quot;tokens&quot;&#10;            }&#10;        ),&#10;    ]&#10;&#10;    # Embed them&#10;    embeddings = embed_chunks(chunks)&#10;&#10;    # Embeddings should be returned&#10;    assert len(embeddings) == 2&#10;&#10;    # Original chunks should still have their metadata intact&#10;    assert chunks[0].metadata[&quot;source_id&quot;] == &quot;tutorial.pdf&quot;&#10;    assert chunks[0].metadata[&quot;page_number&quot;] == 5&#10;    assert chunks[1].metadata[&quot;chunk_index&quot;] == 1&#10;&#10;&#10;def test_embed_chunks_type_error():&#10;    &quot;&quot;&quot;Test that passing wrong type raises TypeError&quot;&quot;&quot;&#10;    with pytest.raises(TypeError, match=&quot;chunks must be Chunk or List\\[Chunk\\]&quot;):&#10;        embed_chunks(&quot;plain string&quot;)&#10;&#10;    with pytest.raises(TypeError, match=&quot;Expected Chunk object&quot;):&#10;        embed_chunks([&quot;string1&quot;, &quot;string2&quot;])&#10;&#10;&#10;def test_step_2_2_criteria():&#10;    &quot;&quot;&quot;&#10;    Verify step 2.2 criteria:&#10;    - Accepts a text chunk (Chunk object) ✓&#10;    - Accepts a list of chunks (List[Chunk]) ✓&#10;    - Returns a vector (List[float]) ✓&#10;    - Returns list of vectors (List[List[float]]) ✓&#10;    - Interacts with model via API ✓ (through embed_text)&#10;    &quot;&quot;&quot;&#10;    # This test documents that all criteria are met&#10;    # The actual tests above verify each criterion&#10;    assert hasattr(embed_chunks, '__call__')&#10;    assert embed_chunks.__doc__ is not None&#10;    assert &quot;Service wrapper&quot; in embed_chunks.__doc__&#10;    assert &quot;Step 2.2&quot; in embed_chunks.__doc__&#10;&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Test for step 2.2: Service wrapper for embedding model&#10;Tests that embed_chunks() accepts Chunk objects and returns vectors&#10;&quot;&quot;&quot;&#10;from typing import List&#10;&#10;import pytest&#10;&#10;from src.data_processing.models import Chunk&#10;from src.core.embeddings import embed_chunks&#10;&#10;&#10;def test_embed_single_chunk(monkeypatch):&#10;    &quot;&quot;&quot;Test embedding a single Chunk object&quot;&quot;&quot;&#10;    # Mock the actual API call&#10;    def fake_embed_text(text: str, **kwargs):&#10;        return [1.0, 2.0, 3.0]&#10;&#10;    monkeypatch.setattr(&quot;core.embeddings.embed_text&quot;, fake_embed_text)&#10;&#10;    # Create a Chunk&#10;    chunk = Chunk(text=&quot;Test chunk text&quot;, metadata={&quot;source&quot;: &quot;test.pdf&quot;, &quot;page&quot;: 1})&#10;&#10;    # Embed it&#10;    result = embed_chunks(chunk)&#10;&#10;    # Should return a single vector&#10;    assert isinstance(result, list)&#10;    assert len(result) == 3&#10;    assert result == [1.0, 2.0, 3.0]&#10;&#10;&#10;def test_embed_multiple_chunks(monkeypatch):&#10;    &quot;&quot;&quot;Test embedding a list of Chunk objects&quot;&quot;&quot;&#10;    # Mock the actual API call&#10;    call_count = [0]&#10;&#10;    def fake_embed_text(text: str, **kwargs):&#10;        call_count[0] += 1&#10;        # Return different vectors based on text length&#10;        return [float(len(text)), float(call_count[0]), 0.5]&#10;&#10;    monkeypatch.setattr(&quot;core.embeddings.embed_text&quot;, fake_embed_text)&#10;&#10;    # Create multiple Chunks&#10;    chunks = [&#10;        Chunk(text=&quot;First chunk&quot;, metadata={&quot;index&quot;: 0}),&#10;        Chunk(text=&quot;Second chunk text&quot;, metadata={&quot;index&quot;: 1}),&#10;        Chunk(text=&quot;Third&quot;, metadata={&quot;index&quot;: 2}),&#10;    ]&#10;&#10;    # Embed them&#10;    result = embed_chunks(chunks)&#10;&#10;    # Should return list of vectors&#10;    assert isinstance(result, list)&#10;    assert len(result) == 3&#10;    assert all(isinstance(vec, list) for vec in result)&#10;&#10;    # Check that each chunk was processed&#10;    assert result[0] == [11.0, 1.0, 0.5]  # len(&quot;First chunk&quot;) = 11&#10;    assert result[1] == [17.0, 2.0, 0.5]  # len(&quot;Second chunk text&quot;) = 17&#10;    assert result[2] == [5.0, 3.0, 0.5]   # len(&quot;Third&quot;) = 5&#10;&#10;&#10;def test_embed_chunks_with_metadata_preserved(monkeypatch):&#10;    &quot;&quot;&quot;Verify that chunk metadata is preserved (not lost during embedding)&quot;&quot;&quot;&#10;    def fake_embed_text(text: str, **kwargs):&#10;        return [1.0, 2.0, 3.0]&#10;&#10;    monkeypatch.setattr(&quot;core.embeddings.embed_text&quot;, fake_embed_text)&#10;&#10;    # Create chunks with rich metadata&#10;    chunks = [&#10;        Chunk(&#10;            text=&quot;Python is great&quot;,&#10;            metadata={&#10;                &quot;source_id&quot;: &quot;tutorial.pdf&quot;,&#10;                &quot;page_number&quot;: 5,&#10;                &quot;chunk_index&quot;: 0,&#10;                &quot;strategy&quot;: &quot;tokens&quot;&#10;            }&#10;        ),&#10;        Chunk(&#10;            text=&quot;Machine learning is powerful&quot;,&#10;            metadata={&#10;                &quot;source_id&quot;: &quot;tutorial.pdf&quot;,&#10;                &quot;page_number&quot;: 6,&#10;                &quot;chunk_index&quot;: 1,&#10;                &quot;strategy&quot;: &quot;tokens&quot;&#10;            }&#10;        ),&#10;    ]&#10;&#10;    # Embed them&#10;    embeddings = embed_chunks(chunks)&#10;&#10;    # Embeddings should be returned&#10;    assert len(embeddings) == 2&#10;&#10;    # Original chunks should still have their metadata intact&#10;    assert chunks[0].metadata[&quot;source_id&quot;] == &quot;tutorial.pdf&quot;&#10;    assert chunks[0].metadata[&quot;page_number&quot;] == 5&#10;    assert chunks[1].metadata[&quot;chunk_index&quot;] == 1&#10;&#10;&#10;def test_embed_chunks_type_error():&#10;    &quot;&quot;&quot;Test that passing wrong type raises TypeError&quot;&quot;&quot;&#10;    with pytest.raises(TypeError, match=&quot;chunks must be Chunk or List\\[Chunk\\]&quot;):&#10;        embed_chunks(&quot;plain string&quot;)&#10;&#10;    with pytest.raises(TypeError, match=&quot;Expected Chunk object&quot;):&#10;        embed_chunks([&quot;string1&quot;, &quot;string2&quot;])&#10;&#10;&#10;def test_step_2_2_criteria():&#10;    &quot;&quot;&quot;&#10;    Verify step 2.2 criteria:&#10;    - Accepts a text chunk (Chunk object) ✓&#10;    - Accepts a list of chunks (List[Chunk]) ✓&#10;    - Returns a vector (List[float]) ✓&#10;    - Returns list of vectors (List[List[float]]) ✓&#10;    - Interacts with model via API ✓ (through embed_text)&#10;    &quot;&quot;&quot;&#10;    # This test documents that all criteria are met&#10;    # The actual tests above verify each criterion&#10;    assert hasattr(embed_chunks, '__call__')&#10;    assert embed_chunks.__doc__ is not None&#10;    assert &quot;Service wrapper&quot; in embed_chunks.__doc__&#10;    assert &quot;Step 2.2&quot; in embed_chunks.__doc__" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>